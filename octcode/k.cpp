4. Which of the following statements are true for Stochastic Gradient Descent (SGD)?
The model weights are updated after going over the full training data in each iteration
The model weights are updated for every single training data point in each iteration
SGD works better than batch GD for small datasets
Convergence path of SGD is less noisy compared to batch GD
